{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "169f990b",
   "metadata": {},
   "source": [
    "### Import libraries and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bb91c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import my favorite imports.\n",
    "import pandas as pd\n",
    "import env\n",
    "import acquire\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.preprocessing\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from scipy import stats\n",
    "\n",
    "# modeling methods\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression, LassoLars, TweedieRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import explained_variance_score\n",
    "\n",
    "import sklearn.linear_model\n",
    "import sklearn.feature_selection\n",
    "import sklearn.preprocessing\n",
    "\n",
    "# import splitting and imputing functions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# turn off pink boxes for demo\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e335a3",
   "metadata": {},
   "source": [
    "## Project Goals:\n",
    "\n",
    "### Explore the 2017 Zillow data and create a model that will be able to predict a property's tax assessed value of single family properties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b4b939",
   "metadata": {},
   "source": [
    "### Acquire data and split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efa4afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acuire data, clean it up, and split into train, validate, and test datasets.\n",
    "train, validate, test = acquire.wrangle_zillow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1324a3",
   "metadata": {},
   "source": [
    "### Verify no null values and datatypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea331d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that each data set does not have null values and that the datatypes are correct.\n",
    "train.info(), validate.info(), test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2e2ed5",
   "metadata": {},
   "source": [
    "### Visualize dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd10231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the 'train' dataset.\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91308470",
   "metadata": {},
   "source": [
    "### Scale data and update dataframe with columns for scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbabd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scaler object using Min-Max Scaler.\n",
    "scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "\n",
    "# Scale the independent variables and add scaled columns to dataframe.\n",
    "train, validate, test = acquire.add_scaled_columns(train, validate, test, scaler, ['sqft', 'bedrooms', 'bathrooms', 'fips'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42699495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize scaled data columns.\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699ac7c3",
   "metadata": {},
   "source": [
    "### Create baseline prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b817a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a baseline prediction as the average of the tax values of all the homes.\n",
    "baseline = train.tax_value.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58c560c",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea324e7",
   "metadata": {},
   "source": [
    "### Create variables for independent variables to test against target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60d67ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"tax_value\"\n",
    "\n",
    "# split train into X (dataframe, drop target) & y (series, keep target only)\n",
    "X_train = train.drop(columns=[target])\n",
    "y_train = train[target]\n",
    "\n",
    "# split validate into X (dataframe, drop target) & y (series, keep target only)\n",
    "X_validate = validate.drop(columns=[target])\n",
    "y_validate = validate[target]\n",
    "\n",
    "# split test into X (dataframe, drop target) & y (series, keep target only)\n",
    "X_test = test.drop(columns=[target])\n",
    "y_test = test[target]\n",
    "\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052a1f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize preview of independent variables' dataframe and make sure target is not included.\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d1beb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns that have the original values of the independent variables.\n",
    "X_train_scaled = X_train.drop(columns=['sqft','bedrooms','bathrooms', 'fips'])\n",
    "X_validate_scaled = X_validate.drop(columns=['sqft','bedrooms','bathrooms', 'fips'])\n",
    "X_test_scaled = X_test.drop(columns=['sqft','bedrooms','bathrooms', 'fips'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafbf9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dataframe of only scaled values.\n",
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd59d8d1",
   "metadata": {},
   "source": [
    "### Identify top two features that influence the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44d732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the 'select_kbest' function housed in the 'acquire' module to identify top two features.\n",
    "acquire.select_kbest(X_train_scaled, y_train, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa056c5c",
   "metadata": {},
   "source": [
    "### Convert arrays into dataframes to make easier to modify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b496323e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.DataFrame(y_train)\n",
    "y_validate = pd.DataFrame(y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af1df69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the mean of the tax value to use as a baseline prediction.\n",
    "tax_value_mean = baseline\n",
    "y_train['tax_value_mean'] = baseline\n",
    "y_validate['tax_value_mean'] = baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18071ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize target value with baseline prediction.\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c7c0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the RMSE value for baseline prediction.\n",
    "rmse_train = mean_squared_error(y_train.tax_value,\n",
    "                                y_train.tax_value_mean) ** .5\n",
    "rmse_validate = mean_squared_error(y_validate.tax_value, y_validate.tax_value_mean) ** (0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee275f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RMSE using Mean\\nTrain/In-Sample: \", round(rmse_train, 2), \n",
    "      \"\\nValidate/Out-of-Sample: \", round(rmse_validate, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c5d2e7",
   "metadata": {},
   "source": [
    "### Create a datafame that will house the name of the model used and the RMSE values for the train and validate datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8addc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df = pd.DataFrame(data=[\n",
    "            {\n",
    "                'model': 'mean_baseline', \n",
    "                'RMSE_train': rmse_train,\n",
    "                'RMSE_validate': rmse_validate\n",
    "                }\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b46c677",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8b2992",
   "metadata": {},
   "source": [
    "### Call the 'evaluate_model' function to update the 'metric_df' with different models and their RMSE values. All of the models used were fit on the scaled values of the independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5303b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df = acquire.model_evaluation(X_train_scaled, X_validate_scaled, y_train, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e79723",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df['difference'] = metric_df.RMSE_validate - metric_df.RMSE_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacb5ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77a9b5c",
   "metadata": {},
   "source": [
    "### Plotting Actual vs. Predicted Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd088dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_validate.head()\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(y_validate.tax_value, y_validate.tax_value_mean, alpha=.5, color=\"gray\", label='_nolegend_')\n",
    "plt.annotate(\"Baseline: Predict Using Mean\", (16, 230000))\n",
    "plt.plot(y_validate.tax_value, y_validate.tax_value, alpha=.5, color=\"blue\", label='_nolegend_')\n",
    "plt.annotate(\"The Ideal Line: Predicted = Actual\", (10, 20000), rotation=26)\n",
    "\n",
    "plt.scatter(y_validate.tax_value, y_validate.tax_value_pred_lm, \n",
    "            alpha=.5, color=\"red\", s=100, label=\"Model: LinearRegression\")\n",
    "plt.scatter(y_validate.tax_value, y_validate.tax_value_pred_glm, \n",
    "            alpha=.5, color=\"yellow\", s=100, label=\"Model: TweedieRegressor\")\n",
    "plt.scatter(y_validate.tax_value, y_validate.tax_value_pred_lars, alpha=.5, color=\"green\", s=100, label=\"Model: LassoLars\")\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Actual Tax Value\")\n",
    "plt.ylabel(\"Predicted Tax Value\")\n",
    "plt.title(\"Where are predictions more extreme? More modest?\")\n",
    "# plt.annotate(\"The polynomial model appears to overreact to noise\", (2.0, -10))\n",
    "# plt.annotate(\"The OLS model (LinearRegression)\\n appears to be most consistent\", (15.5, 3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8711c6",
   "metadata": {},
   "source": [
    "### Going with the OLS model to use on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d51a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict test\n",
    "# use the thing!\n",
    "lm = LinearRegression()\n",
    "# fit the thing\n",
    "lm.fit(X_train_scaled, y_train.tax_value)\n",
    "\n",
    "y_test_predictions = lm.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d542dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate: rmse\n",
    "rmse_test = mean_squared_error(y_test, y_test_predictions) ** (1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e57e7c7",
   "metadata": {},
   "source": [
    "### Compare OLS model with baseline prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f302301",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('OLS RMSE: ', rmse_test)\n",
    "metric_df.drop(columns='difference')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d617f01",
   "metadata": {},
   "source": [
    "### Conclusions & Recommendations\n",
    "\n",
    "* With a baseline RMSE of 197,243 and my model's RMSE of 187,977 my model is a slight improvement on the baseline model.\n",
    "* At this time I do not feel confident in using my model to predict the tax value of a property."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af28463",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "* With more time I would like to investigate different features and how they influence the target to improve my current OLS model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ff5e66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
